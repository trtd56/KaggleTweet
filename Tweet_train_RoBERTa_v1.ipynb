{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweet_train_RoBERTa.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxf_9P9G3-FZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P4lu2bU39G-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHjixEwh4B5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip uninstall -y tensorflow\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip list | grep -E 'transformers|tokenizers'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix0cy6nz4H8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "from scipy.stats import norm\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.utils.data\n",
        "\n",
        "from transformers import *\n",
        "from transformers import BertTokenizer\n",
        "import tokenizers\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "from math import floor, ceil\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "sns.set()\n",
        "device = torch.device('cuda')\n",
        "\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "def get_best_start_end_idxs(_start_logits, _end_logits):\n",
        "    best_logit = -1000\n",
        "    best_idxs = None\n",
        "    for start_idx, start_logit in enumerate(_start_logits):\n",
        "        for end_idx, end_logit in enumerate(_end_logits[start_idx:]):\n",
        "            logit_sum = (start_logit + end_logit).item()\n",
        "            if logit_sum > best_logit:\n",
        "                best_logit = logit_sum\n",
        "                best_idxs = (start_idx, start_idx+end_idx)\n",
        "    return best_idxs\n",
        "\n",
        "def calculate_jaccard_score(\n",
        "    original_tweet, \n",
        "    target_string, \n",
        "    sentiment_val, \n",
        "    idx_start, \n",
        "    idx_end, \n",
        "    offsets,\n",
        "    verbose=False):\n",
        "    \n",
        "    if idx_end < idx_start:\n",
        "        idx_end = idx_start\n",
        "    \n",
        "    filtered_output  = \"\"\n",
        "    for ix in range(idx_start, idx_end + 1):\n",
        "        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
        "        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
        "            filtered_output += \" \"\n",
        "\n",
        "    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n",
        "        filtered_output = original_tweet\n",
        "\n",
        "\n",
        "    jac = jaccard(target_string.strip(), filtered_output.strip())\n",
        "    return jac, filtered_output\n",
        "\n",
        "\n",
        "class config:\n",
        "    SEED = 43875210\n",
        "    MAX_LEN = 128\n",
        "    TRAIN_BATCH_SIZE = 32\n",
        "    VALID_BATCH_SIZE = 8\n",
        "    LR = 3e-5\n",
        "    N_FOLDS = 5\n",
        "    EPOCHS = 3\n",
        "    PRETRAINED_WEIGHTS = \"roberta-base\"\n",
        "    DRIVE_ROOT = \"./drive/My Drive/Study/Tweet\"\n",
        "    TRAIN_PATH = f\"{DRIVE_ROOT}/input/all_tweet_data.csv\"\n",
        "    MODEL_PATH = f\"{DRIVE_ROOT}/model/roberta_base\"\n",
        "    TOKENIZER = tokenizers.ByteLevelBPETokenizer(vocab_file=f\"{MODEL_PATH}/vocab.json\", merges_file=f\"{MODEL_PATH}/merges.txt\", lowercase=True, add_prefix_space=True)\n",
        "    UUID = \"RoBERTa_base_21\"\n",
        "    OUT_DIR = f\"{DRIVE_ROOT}/output/{UUID}/\"\n",
        "    THR = 0.5\n",
        "\n",
        "!mkdir -p \"{config.OUT_DIR}\"\n",
        "\n",
        "sentiment_id = {s: config.TOKENIZER.encode(s).ids[0] for s in [\"positive\", \"negative\", \"neutral\"]}\n",
        "print(sentiment_id)\n",
        "\n",
        "#import json\n",
        "#with open(f\"{config.MODEL_PATH}/vocab.json\", \"r\") as f:\n",
        "#    d = json.load(f)\n",
        "#d[\"<mask>\"]\n",
        "MASK_ID = 50264\n",
        "\n",
        "all_tweet_data = pd.read_csv(config.TRAIN_PATH)\n",
        "train = all_tweet_data[all_tweet_data[\"data_type\"] == \"train\"].reset_index(drop=True)\n",
        "print(\"origin\", len(train))\n",
        "train = train[train[\"is_keep\"]].reset_index(drop=True)\n",
        "print(\"del duplicates\", len(train))\n",
        "train = train[[\"text\", \"selected_text\", \"new_sentiment\", \"sentiment\"]]\n",
        "train.columns = [\"text\", \"selected_text\", \"sentiment\", \"org_sentiment\"]\n",
        "\n",
        "test = all_tweet_data[all_tweet_data[\"data_type\"] != \"train\"].reset_index(drop=True)\n",
        "print(test.shape[0])\n",
        "test = test[test[\"is_keep\"]].reset_index(drop=True)\n",
        "test = test[test[\"text\"].map(lambda x: len(x)) > 3].reset_index(drop=True)\n",
        "test = test[test[\"text\"].map(lambda x: len(set([xx for xx in x if xx not in [\" \", \".\", \"!\", \"?\", \"_\"]])) > 2 )].reset_index(drop=True)\n",
        "print(test.shape[0])\n",
        "test = test[[\"text\", \"text\", \"assumed_sentiment\", \"sentiment\"]]\n",
        "test.columns = [\"text\", \"selected_text\", \"sentiment\", \"org_sentiment\"]\n",
        "\n",
        "sentiment_label = {v: [config.TOKENIZER.encode(v).ids[0]] for v in train[\"org_sentiment\"].unique()}\n",
        "sentiment_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luEZbBWh4MtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_data(tweet, selected_text, sentiment, tokenizer, max_len, org_sentiment):\n",
        "    tweet = \" \" + \" \".join(str(tweet).split())\n",
        "    selected_text = \" \" + \" \".join(str(selected_text).split())\n",
        "\n",
        "    len_st = len(selected_text) - 1\n",
        "    idx0 = None\n",
        "    idx1 = None\n",
        "\n",
        "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
        "        if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
        "            idx0 = ind\n",
        "            idx1 = ind + len_st - 1\n",
        "            break\n",
        "\n",
        "    char_targets = [0] * len(tweet)\n",
        "    if idx0 != None and idx1 != None:\n",
        "        for ct in range(idx0, idx1 + 1):\n",
        "            char_targets[ct] = 1\n",
        "\n",
        "    tok_tweet = tokenizer.encode(tweet)\n",
        "    input_ids_orig = tok_tweet.ids\n",
        "    tweet_offsets = tok_tweet.offsets\n",
        "    \n",
        "    target_idx = []\n",
        "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
        "        if sum(char_targets[offset1: offset2]) > 0:\n",
        "            target_idx.append(j)\n",
        "\n",
        "    try:\n",
        "        targets_start = target_idx[0]\n",
        "        targets_end = target_idx[-1]\n",
        "    except IndexError:\n",
        "        targets_start = 0\n",
        "        targets_end = len(tweet) - 1     \n",
        "\n",
        "\n",
        "    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n",
        "    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n",
        "    mask = [1] * len(token_type_ids)\n",
        "    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n",
        "    targets_start += 4\n",
        "    targets_end += 4\n",
        "\n",
        "    padding_length = max_len - len(input_ids)\n",
        "    if padding_length > 0:\n",
        "        input_ids = input_ids + ([1] * padding_length)\n",
        "        mask = mask + ([0] * padding_length)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
        "    \n",
        "    return {\n",
        "        'ids': input_ids,\n",
        "        'mask': mask,\n",
        "        'token_type_ids': token_type_ids,\n",
        "        'targets_start': targets_start,\n",
        "        'targets_end': targets_end,\n",
        "        'orig_tweet': tweet,\n",
        "        'orig_selected': selected_text,\n",
        "        'sentiment': sentiment,\n",
        "        'offsets': tweet_offsets,\n",
        "        'sentiment_label': sentiment_label[org_sentiment]\n",
        "    }\n",
        "\n",
        "class TweetDataset:\n",
        "    def __init__(self, tweet, sentiment, selected_text, org_sentiment):\n",
        "        self.tweet = tweet\n",
        "        self.sentiment = sentiment\n",
        "        self.selected_text = selected_text\n",
        "        self.tokenizer = config.TOKENIZER\n",
        "        self.max_len = config.MAX_LEN\n",
        "        self.org_sentiment = org_sentiment\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tweet)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        data = process_data(\n",
        "            self.tweet[item], \n",
        "            self.selected_text[item],\n",
        "            self.sentiment[item],\n",
        "            self.tokenizer,\n",
        "            self.max_len,\n",
        "            self.org_sentiment[item]\n",
        "        )\n",
        "        \n",
        "        max_idx = data[\"mask\"].index(0)\n",
        "        masked_idx = random.sample(list(range(4, max_idx)), 2)\n",
        "\n",
        "        dummy_ids = [MASK_ID if i in masked_idx else v for i, v in enumerate(data[\"ids\"])]\n",
        "        dummy_target = [v if i in masked_idx else -1 for i, v in enumerate(data[\"ids\"])]\n",
        "        \n",
        "        dummy_target[1] = data[\"sentiment_label\"][0]\n",
        "\n",
        "        bce_target = torch.zeros(self.max_len)\n",
        "        bce_target[data[\"targets_start\"]:data[\"targets_end\"]+1] = 1.0\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
        "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
        "            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n",
        "            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n",
        "            'orig_tweet': data[\"orig_tweet\"],\n",
        "            'orig_selected': data[\"orig_selected\"],\n",
        "            'sentiment': data[\"sentiment\"],\n",
        "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long),\n",
        "            'sentiment_label': torch.tensor(data[\"sentiment_label\"], dtype=torch.long),\n",
        "            'dummy_ids': torch.tensor(dummy_ids, dtype=torch.long),\n",
        "            'dummy_target': torch.tensor(dummy_target, dtype=torch.long),\n",
        "            'bce_target': torch.tensor(bce_target, dtype=torch.float),\n",
        "        }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aTt45vq4Pog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_fn(data_loader, model, optimizer, device, scheduler=None):\n",
        "    model.train()\n",
        "    losses, lrs = [], []\n",
        "    tk0 = tqdm(data_loader, total=len(data_loader), desc=\"Training\")\n",
        "    for bi, d in enumerate(tk0):\n",
        "\n",
        "        ids = d[\"ids\"]\n",
        "        token_type_ids = d[\"token_type_ids\"]\n",
        "        mask = d[\"mask\"]\n",
        "        targets_start = d[\"targets_start\"]\n",
        "        targets_end = d[\"targets_end\"]\n",
        "        sentiment = d[\"sentiment\"]\n",
        "        orig_selected = d[\"orig_selected\"]\n",
        "        orig_tweet = d[\"orig_tweet\"]\n",
        "        targets_start = d[\"targets_start\"]\n",
        "        targets_end = d[\"targets_end\"]\n",
        "        offsets = d[\"offsets\"]\n",
        "        sentiment_label = d[\"sentiment_label\"]\n",
        "        dummy_ids = d[\"dummy_ids\"]\n",
        "        dummy_target = d[\"dummy_target\"]\n",
        "        bce_target = d[\"bce_target\"]\n",
        "\n",
        "        ids = ids.to(device, dtype=torch.long)\n",
        "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        mask = mask.to(device, dtype=torch.long)\n",
        "        targets_start = targets_start.to(device, dtype=torch.long)\n",
        "        targets_end = targets_end.to(device, dtype=torch.long)\n",
        "        sentiment_label = sentiment_label.to(device, dtype=torch.long)\n",
        "        dummy_ids = dummy_ids.to(device, dtype=torch.long)\n",
        "        dummy_target = dummy_target.to(device, dtype=torch.long)\n",
        "        bce_target = bce_target.to(device, dtype=torch.float)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs_start, outputs_end, outputs, lm = model(\n",
        "        #outputs, lm = model(\n",
        "        #outputs_start, outputs_end, lm = model(\n",
        "        #outputs_start, outputs_end = model(\n",
        "            ids=ids,\n",
        "            mask=mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            is_train = True,\n",
        "            dummy_ids=dummy_ids,\n",
        "        )\n",
        "        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end, outputs, bce_target, lm, dummy_target)\n",
        "        #loss = loss_fn(outputs, bce_target, lm, dummy_target)\n",
        "        #loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end, lm, dummy_target)\n",
        "        #loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        tk0.set_postfix(loss=loss.item())\n",
        "\n",
        "        losses.append(float(loss))\n",
        "        lrs.append(np.array([param_group[\"lr\"] for param_group in optimizer.param_groups]).mean())\n",
        "\n",
        "    return losses, lrs\n",
        "\n",
        "\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    jaccards = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        tk0 = tqdm(data_loader, total=len(data_loader))\n",
        "        for bi, d in enumerate(tk0):\n",
        "            ids = d[\"ids\"]\n",
        "            token_type_ids = d[\"token_type_ids\"]\n",
        "            mask = d[\"mask\"]\n",
        "            sentiment = d[\"sentiment\"]\n",
        "            orig_selected = d[\"orig_selected\"]\n",
        "            orig_tweet = d[\"orig_tweet\"]\n",
        "            targets_start = d[\"targets_start\"]\n",
        "            targets_end = d[\"targets_end\"]\n",
        "            offsets = d[\"offsets\"].numpy()\n",
        "            sentiment_label = d[\"sentiment_label\"]\n",
        "\n",
        "            ids = ids.to(device, dtype=torch.long)\n",
        "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "            targets_start = targets_start.to(device, dtype=torch.long)\n",
        "            targets_end = targets_end.to(device, dtype=torch.long)\n",
        "            sentiment_label = sentiment_label.to(device, dtype=torch.long)\n",
        "\n",
        "            #outputs = model(\n",
        "            outputs_start, outputs_end = model(\n",
        "                ids=ids,\n",
        "                mask=mask,\n",
        "                token_type_ids=token_type_ids\n",
        "            )\n",
        "            \n",
        "            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
        "            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
        "\n",
        "            \"\"\"\n",
        "            output_sigmoid = outputs.sigmoid().detach().cpu().numpy()\n",
        "            pos_index = [np.where(o > config.THR)[0] for o in output_sigmoid]\n",
        "            outputs_start = []\n",
        "            outputs_end = []\n",
        "            for idx in pos_index:\n",
        "                try:\n",
        "                    h = 4 if idx[0] < 4 else idx[0]\n",
        "                except IndexError:\n",
        "                    h = 4\n",
        "                try:\n",
        "                    t = idx[-1]\n",
        "                except IndexError:\n",
        "                    t = config.MAX_LEN - 1\n",
        "                outputs_start.append(h)\n",
        "                outputs_end.append(t)\n",
        "            \"\"\"\n",
        "\n",
        "            jaccard_scores = []\n",
        "            for px, tweet in enumerate(orig_tweet):\n",
        "                selected_tweet = orig_selected[px]\n",
        "                tweet_sentiment = sentiment[px]\n",
        "                idx_start, idx_end = get_best_start_end_idxs(outputs_start[px, :], outputs_end[px, :])\n",
        "                #idx_start, idx_end = outputs_start[px], outputs_end[px]\n",
        "                jaccard_score, _ = calculate_jaccard_score(\n",
        "                    original_tweet=tweet,\n",
        "                    target_string=selected_tweet,\n",
        "                    sentiment_val=tweet_sentiment,\n",
        "                    idx_start=idx_start,\n",
        "                    idx_end=idx_end,\n",
        "                    offsets=offsets[px]\n",
        "                )\n",
        "                jaccard_scores.append(jaccard_score)\n",
        "\n",
        "            jaccards.append(np.mean(jaccard_scores))\n",
        "            losses.append(0.0)\n",
        "    \n",
        "    return np.mean(losses), np.mean(jaccards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDiDyFVF4RgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(start_logits, end_logits, start_positions, end_positions, outputs, bce_target, lm, ids):\n",
        "#def loss_fn(outputs, bce_target, lm, ids):\n",
        "#def loss_fn(start_logits, end_logits, start_positions, end_positions, lm, ids):\n",
        "#def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
        "    loss_fct = nn.CrossEntropyLoss()\n",
        "    start_loss = loss_fct(start_logits, start_positions)\n",
        "    end_loss = loss_fct(end_logits, end_positions)\n",
        "\n",
        "    bce_loss = nn.BCEWithLogitsLoss()(outputs, bce_target)\n",
        "    lm_loss = nn.CrossEntropyLoss(ignore_index=-1)(lm, ids.view(-1))\n",
        "    \n",
        "    #total_loss = start_loss + end_loss + lm_loss\n",
        "    total_loss = bce_loss + lm_loss + start_loss + end_loss\n",
        "    return total_loss\n",
        "    \n",
        "class RoBERTaForTweet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RoBERTaForTweet, self).__init__()\n",
        "\n",
        "        conf = RobertaConfig.from_pretrained(config.MODEL_PATH)\n",
        "        conf.output_hidden_states = True\n",
        "\n",
        "        self.n_vocab = conf.vocab_size\n",
        "\n",
        "        self.roberta = RobertaModel.from_pretrained(config.MODEL_PATH, config=conf)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "        n_weights = conf.num_hidden_layers + 1\n",
        "        weights_init = torch.zeros(n_weights).float()\n",
        "        weights_init.data[:-1] = -3\n",
        "        self.layer_weights = torch.nn.Parameter(weights_init)\n",
        "\n",
        "        self.dropouts = nn.ModuleList([nn.Dropout(0.2) for _ in range(5)])\n",
        "        self.qa_outputs1 = nn.Linear(768, 2)\n",
        "        self.qa_outputs2 = nn.Linear(768, 1)\n",
        "\n",
        "        self.lm_outputs = modeling_roberta.RobertaLMHead(conf)\n",
        "\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids, is_train=False, dummy_ids=None):\n",
        "        _, _, out = self.roberta(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
        "\n",
        "        sequence_output = torch.stack([self.dropout(layer) for layer in out], dim=3)\n",
        "        sequence_output = (torch.softmax(self.layer_weights, dim=0) * sequence_output).sum(-1)\n",
        "        #logits = sum([self.qa_outputs(dropout(sequence_output)) for dropout in self.dropouts])/5\n",
        "\n",
        "        logits1 = sum([self.qa_outputs1(dropout(sequence_output)) for dropout in self.dropouts])/5\n",
        "        logits2 = sum([self.qa_outputs2(dropout(sequence_output)) for dropout in self.dropouts])/5\n",
        "\n",
        "        logits = logits1\n",
        "        logits2 = logits2.squeeze(-1)\n",
        "\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        if is_train:\n",
        "            lm_outputs, _, _ = self.roberta(dummy_ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
        "            lm = self.lm_outputs(lm_outputs)\n",
        "            lm = lm.view(-1, self.n_vocab)\n",
        "\n",
        "            #return start_logits, end_logits, lm\n",
        "            #return logits, lm\n",
        "            return start_logits, end_logits, logits2, lm\n",
        "\n",
        "        return start_logits, end_logits\n",
        "        #return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moUYtjxj4Tbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "skf = StratifiedKFold(n_splits=config.N_FOLDS, shuffle=True, random_state=config.SEED).split(train[\"sentiment\"].values, train[\"sentiment\"].values)\n",
        "\n",
        "train_log_dfs, valid_log_dfs = [], []\n",
        "for fold, (train_idx, valid_idx) in enumerate(skf):\n",
        "    print(f\"### Fold-{fold} ###\")\n",
        "    #if fold in [0, 1, 2]:\n",
        "    #    continue\n",
        "    seed_everything(config.SEED + fold)\n",
        "\n",
        "    df_train = train.iloc[train_idx].reset_index(drop=True)\n",
        "    df_valid = train.iloc[valid_idx].reset_index(drop=True)\n",
        "\n",
        "    train_dataset = TweetDataset(\n",
        "        tweet=df_train.text.values,\n",
        "        sentiment=df_train.sentiment.values,\n",
        "        selected_text=df_train.selected_text.values,\n",
        "        org_sentiment=df_train.org_sentiment.values\n",
        "    )\n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.TRAIN_BATCH_SIZE,\n",
        "        num_workers=0,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    valid_dataset = TweetDataset(\n",
        "        tweet=df_valid.text.values,\n",
        "        sentiment=df_valid.sentiment.values,\n",
        "        selected_text=df_valid.selected_text.values,\n",
        "        org_sentiment=df_valid.org_sentiment.values\n",
        "    )\n",
        "\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=config.VALID_BATCH_SIZE,\n",
        "        num_workers=0,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    model = RoBERTaForTweet()\n",
        "    model.to(device)\n",
        "\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_parameters, lr=config.LR)\n",
        "    \n",
        "    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n",
        "    \n",
        "    t_res, v_res = [], []\n",
        "    best_score = 0\n",
        "    for epoch in range(config.EPOCHS):\n",
        "        train_loss, lrs = train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n",
        "        valid_loss, valid_jaccard = eval_fn(valid_data_loader, model, device)\n",
        "        print(f\"{epoch} epoch, jaccard={valid_jaccard}\")\n",
        "        if best_score < valid_jaccard:\n",
        "            print(f\"  --> Best Model Update!!\")\n",
        "            best_score = valid_jaccard\n",
        "            torch.save(model.state_dict(), f\"{config.OUT_DIR}/roberta_f{fold}_best.bin\")\n",
        "        t_res.append(pd.DataFrame(zip(train_loss, lrs), columns=[\"train_loss\", \"lrs\"]))\n",
        "        v_res.append((valid_loss, valid_jaccard))\n",
        "\n",
        "    train_log_dfs.append(pd.concat(t_res, axis=0).reset_index(drop=True))\n",
        "    valid_log_dfs.append(pd.DataFrame(v_res, columns=[\"valid_loss\", \"jaccard_score\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teu7ehHjM5X5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjImPBJXtFvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}