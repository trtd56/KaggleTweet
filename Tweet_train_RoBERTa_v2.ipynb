{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweet_train_RoBERTa.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxf_9P9G3-FZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P4lu2bU39G-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHjixEwh4B5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip list | grep -E 'transformers|tokenizers'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKTF369sd4or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "from scipy.stats import norm\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.utils.data\n",
        "\n",
        "from transformers import *\n",
        "from transformers import BertTokenizer\n",
        "import tokenizers\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "from math import floor, ceil\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "sns.set()\n",
        "device = torch.device('cuda')\n",
        "\n",
        "class config:\n",
        "    SEED = 43875210\n",
        "    MAX_LEN = 128\n",
        "    TRAIN_BATCH_SIZE = 64\n",
        "    VALID_BATCH_SIZE = 32\n",
        "    LR = 3e-5\n",
        "    N_FOLDS = 5\n",
        "    EPOCHS = 3\n",
        "    PRETRAINED_WEIGHTS = \"roberta-base\"\n",
        "    DRIVE_ROOT = \"./drive/My Drive/Study/Tweet\"\n",
        "    TRAIN_PATH = f\"{DRIVE_ROOT}/input/all_tweet_data.csv\"\n",
        "    MODEL_PATH = f\"{DRIVE_ROOT}/model/roberta_base\"\n",
        "    TOKENIZER = tokenizers.ByteLevelBPETokenizer(vocab_file=f\"{MODEL_PATH}/vocab.json\", merges_file=f\"{MODEL_PATH}/merges.txt\", lowercase=True, add_prefix_space=True)\n",
        "    UUID = \"RoBERTa_base_41\"\n",
        "    OUT_DIR = f\"{DRIVE_ROOT}/output/{UUID}/\"\n",
        "\n",
        "!mkdir -p \"{config.OUT_DIR}\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix0cy6nz4H8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "def get_best_start_end_idxs(_start_logits, _end_logits):\n",
        "    best_logit = -1000\n",
        "    best_idxs = None\n",
        "    for start_idx, start_logit in enumerate(_start_logits):\n",
        "        for end_idx, end_logit in enumerate(_end_logits[start_idx:]):\n",
        "            logit_sum = (start_logit + end_logit).item()\n",
        "            if logit_sum > best_logit:\n",
        "                best_logit = logit_sum\n",
        "                best_idxs = (start_idx, start_idx+end_idx)\n",
        "    return best_idxs\n",
        "\n",
        "def calculate_jaccard_score(\n",
        "    original_tweet, \n",
        "    target_string, \n",
        "    sentiment_val, \n",
        "    idx_start, \n",
        "    idx_end, \n",
        "    offsets,\n",
        "    verbose=False):\n",
        "    \n",
        "    if idx_end < idx_start:\n",
        "        idx_end = idx_start\n",
        "    \n",
        "    filtered_output  = \"\"\n",
        "    for ix in range(idx_start, idx_end + 1):\n",
        "        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
        "        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
        "            filtered_output += \" \"\n",
        "\n",
        "    if len(original_tweet.split()) < 2:\n",
        "        filtered_output = original_tweet\n",
        "\n",
        "    filtered_output = filtered_output.replace(\"¿¿\", \"¿\").replace(\"ïï\", \"ï\")\n",
        "    jac = jaccard(target_string.strip(), filtered_output.strip())\n",
        "    return jac, filtered_output\n",
        "\n",
        "\n",
        "sentiment_id = {s: config.TOKENIZER.encode(s).ids[0] for s in [\"positive\", \"negative\", \"neutral\"]}\n",
        "print(sentiment_id)\n",
        "\n",
        "\n",
        "all_tweet_data = pd.read_csv(config.TRAIN_PATH)\n",
        "train = all_tweet_data[all_tweet_data[\"data_type\"] == \"train\"].reset_index(drop=True)\n",
        "print(\"origin\", len(train))\n",
        "train = train[train[\"is_keep\"]].reset_index(drop=True)\n",
        "print(\"del duplicates\", len(train))\n",
        "train = train[[\"text\", \"selected_text\", \"new_sentiment\", \"sentiment\"]]\n",
        "train.columns = [\"text\", \"selected_text\", \"sentiment\", \"org_sentiment\"]\n",
        "\n",
        "#sentiment_label = {v: i for i, v in enumerate(train[\"org_sentiment\"].unique())}\n",
        "#sentiment_label = {v: config.TOKENIZER.encode(v).ids[0] for v in train[\"org_sentiment\"].unique()}\n",
        "#sentiment_label = {v: i for i, v in enumerate(train[\"sentiment\"].unique())}\n",
        "#sentiment_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BFuEg7rQTzl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "def filter(text):\n",
        "    # \"RT @user:\"を削除\n",
        "    if \"RT \" in text:\n",
        "        text = text.split(\":\", 1)[1]\n",
        "    # \"@user\"を削除\n",
        "    if \"@\" in text and \" \" in text:\n",
        "        text = text.split(\" \", text.count(\"@\"))[-1]\n",
        "    # \"#tag\"を削除\n",
        "    if \"#\" in text:\n",
        "        text = text.split(\"#\", 1)[0]\n",
        "    # \"URL\"を削除\n",
        "    if \"http\" in text:\n",
        "        text = text.split(\"http\", 1)[0]\n",
        "    return text\n",
        "    \n",
        "big_tweet_data = pd.read_csv(\"./drive/My Drive/Study/Tweet/input/training.1600000.processed.noemoticon.csv\",\n",
        "                             encoding=\"ISO-8859-1\", names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])\n",
        "big_tweet_preds = pd.read_csv(\"./drive/My Drive/Study/Tweet/input/sentiment140_0_10000.csv\")\n",
        "\n",
        "decode_map = {0: \"negative\", 2: \"neutral\", 4: \"positive\"}\n",
        "big_tweet_data[\"sentiment\"] = big_tweet_data[\"target\"].map(decode_map)\n",
        "big_tweet_data = big_tweet_data[[\"ids\", \"text\", \"sentiment\"]]\n",
        "big_tweet_data.columns = [\"textID\", \"text\", \"sentiment\"]\n",
        "\n",
        "nega_df = big_tweet_data[big_tweet_data[\"sentiment\"] == \"negative\"].head(10000).tail(10000)\n",
        "posi_df = big_tweet_data[big_tweet_data[\"sentiment\"] == \"positive\"].head(10000).tail(10000)\n",
        "nega_df[\"text\"] = nega_df[\"text\"].map(filter)\n",
        "posi_df[\"text\"] = posi_df[\"text\"].map(filter)\n",
        "df = pd.concat([posi_df, nega_df], axis=0).reset_index(drop=True)\n",
        "\n",
        "#big_data_df = pd.merge(big_tweet_preds, df, on='textID')\n",
        "big_data_df = pd.concat([big_tweet_preds, df], axis=1)\n",
        "big_data_df[\"org_sentiment\"] = \"empty\"\n",
        "\n",
        "big_data_df = big_data_df[big_data_df[\"text\"].map(lambda x: len(x) > 4)].reset_index(drop=True)\n",
        "\n",
        "big_data_df = big_data_df.sample(10000, random_state=config.SEED)\n",
        "\n",
        "print(big_data_df.shape)\n",
        "big_data_df.head()\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C-aDlaT_LMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#blob_train = pd.read_csv(\"./drive/My Drive/Study/Tweet/input/blob_train.csv\")\n",
        "#print(blob_train.shape, train.shape)\n",
        "#display(blob_train.head())\n",
        "#train = blob_train\n",
        "\n",
        "#pseudo = pd.read_csv(\"./drive/My Drive/Study/Tweet/input/lb0.715_submission.csv\")\n",
        "#test = pd.read_csv(\"./drive/My Drive/Study/Tweet/input/test.csv\")\n",
        "#pseudo = pd.merge(test, pseudo, on='textID')\n",
        "#senti_dict = {k: v for k, v in all_tweet_data[[\"aux_id\", \"sentiment\"]].values}\n",
        "#pseudo[\"org_sentiment\"] = pseudo[\"textID\"].map(senti_dict)\n",
        "\n",
        "pseudo = pd.read_csv(\"./drive/My Drive/Study/Tweet/input/lb0.715_submission_private.csv\")\n",
        "private = all_tweet_data[all_tweet_data[\"data_type\"] != \"train\"].reset_index(drop=True)\n",
        "pseudo.columns = [\"aux_id\", \"selected_text\"]\n",
        "pseudo = pd.merge(private[[\"aux_id\", \"assumed_sentiment\", \"text\"]], pseudo, on='aux_id')\n",
        "senti_dict = {k: v for k, v in all_tweet_data[[\"aux_id\", \"sentiment\"]].values}\n",
        "pseudo[\"org_sentiment\"] = pseudo[\"aux_id\"].map(senti_dict)\n",
        "pseudo.columns = [\"textID\", \"sentiment\", \"text\", \"selected_text\", \"org_sentiment\"]\n",
        "\n",
        "print(pseudo.shape)\n",
        "pseudo.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luEZbBWh4MtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_data(tweet, selected_text, sentiment, tokenizer, max_len, org_sentiment):\n",
        "    tweet = \" \" + \" \".join(str(tweet).split())\n",
        "    selected_text = \" \" + \" \".join(str(selected_text).split())\n",
        "\n",
        "    len_st = len(selected_text) - 1\n",
        "    idx0 = None\n",
        "    idx1 = None\n",
        "\n",
        "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
        "        if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
        "            idx0 = ind\n",
        "            idx1 = ind + len_st - 1\n",
        "            break\n",
        "\n",
        "    char_targets = [0] * len(tweet)\n",
        "    if idx0 != None and idx1 != None:\n",
        "        for ct in range(idx0, idx1 + 1):\n",
        "            char_targets[ct] = 1\n",
        "\n",
        "    tok_tweet = tokenizer.encode(tweet)\n",
        "    input_ids_orig = tok_tweet.ids\n",
        "    tweet_offsets = tok_tweet.offsets\n",
        "    \n",
        "    target_idx = []\n",
        "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
        "        if sum(char_targets[offset1: offset2]) > 0:\n",
        "            target_idx.append(j)\n",
        "\n",
        "    try:\n",
        "        targets_start = target_idx[0]\n",
        "        targets_end = target_idx[-1]\n",
        "    except IndexError:\n",
        "        targets_start = 0\n",
        "        targets_end = len(tweet) - 1     \n",
        "\n",
        "\n",
        "    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n",
        "    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n",
        "    mask = [1] * len(token_type_ids)\n",
        "    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n",
        "    targets_start += 4\n",
        "    targets_end += 4\n",
        "\n",
        "    padding_length = max_len - len(input_ids)\n",
        "    if padding_length > 0:\n",
        "        input_ids = input_ids + ([1] * padding_length)\n",
        "        mask = mask + ([0] * padding_length)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
        "    \n",
        "    return {\n",
        "        'ids': input_ids,\n",
        "        'mask': mask,\n",
        "        'token_type_ids': token_type_ids,\n",
        "        'targets_start': targets_start,\n",
        "        'targets_end': targets_end,\n",
        "        'orig_tweet': tweet,\n",
        "        'orig_selected': selected_text,\n",
        "        'sentiment': sentiment,\n",
        "        'offsets': tweet_offsets,\n",
        "        #'sentiment_label': sentiment_label[org_sentiment]\n",
        "        #'sentiment_label': sentiment_label[sentiment]\n",
        "    }\n",
        "\n",
        "class TweetDataset:\n",
        "    def __init__(self, tweet, sentiment, selected_text, org_sentiment):\n",
        "        self.tweet = tweet\n",
        "        self.sentiment = sentiment\n",
        "        self.selected_text = selected_text\n",
        "        self.tokenizer = config.TOKENIZER\n",
        "        self.max_len = config.MAX_LEN\n",
        "        self.org_sentiment = org_sentiment\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tweet)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        data = process_data(\n",
        "            self.tweet[item], \n",
        "            self.selected_text[item],\n",
        "            self.sentiment[item],\n",
        "            self.tokenizer,\n",
        "            self.max_len,\n",
        "            self.org_sentiment[item]\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
        "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
        "            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n",
        "            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n",
        "            'orig_tweet': data[\"orig_tweet\"],\n",
        "            'orig_selected': data[\"orig_selected\"],\n",
        "            'sentiment': data[\"sentiment\"],\n",
        "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long),\n",
        "            #'sentiment_label': torch.tensor(data[\"sentiment_label\"], dtype=torch.long),\n",
        "        }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aTt45vq4Pog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_fn(data_loader, model, optimizer, device, scheduler=None):\n",
        "    model.train()\n",
        "    losses, lrs = [], []\n",
        "    tk0 = tqdm(data_loader, total=len(data_loader), desc=\"Training\")\n",
        "    for bi, d in enumerate(tk0):\n",
        "\n",
        "        ids = d[\"ids\"]\n",
        "        token_type_ids = d[\"token_type_ids\"]\n",
        "        mask = d[\"mask\"]\n",
        "        targets_start = d[\"targets_start\"]\n",
        "        targets_end = d[\"targets_end\"]\n",
        "        sentiment = d[\"sentiment\"]\n",
        "        orig_selected = d[\"orig_selected\"]\n",
        "        orig_tweet = d[\"orig_tweet\"]\n",
        "        targets_start = d[\"targets_start\"]\n",
        "        targets_end = d[\"targets_end\"]\n",
        "        offsets = d[\"offsets\"]\n",
        "        #sentiment_label = d[\"sentiment_label\"]\n",
        "\n",
        "        ids = ids.to(device, dtype=torch.long)\n",
        "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        mask = mask.to(device, dtype=torch.long)\n",
        "        targets_start = targets_start.to(device, dtype=torch.long)\n",
        "        targets_end = targets_end.to(device, dtype=torch.long)\n",
        "        #sentiment_label = sentiment_label.to(device, dtype=torch.long)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs_start, outputs_end = model(\n",
        "            ids=ids,\n",
        "            mask=mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "        )\n",
        "        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        tk0.set_postfix(loss=loss.item())\n",
        "\n",
        "        losses.append(float(loss))\n",
        "        lrs.append(np.array([param_group[\"lr\"] for param_group in optimizer.param_groups]).mean())\n",
        "\n",
        "    return losses, lrs\n",
        "\n",
        "\n",
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    jaccards = []\n",
        "    final_output = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        tk0 = tqdm(data_loader, total=len(data_loader))\n",
        "        for bi, d in enumerate(tk0):\n",
        "            ids = d[\"ids\"]\n",
        "            token_type_ids = d[\"token_type_ids\"]\n",
        "            mask = d[\"mask\"]\n",
        "            sentiment = d[\"sentiment\"]\n",
        "            orig_selected = d[\"orig_selected\"]\n",
        "            orig_tweet = d[\"orig_tweet\"]\n",
        "            targets_start = d[\"targets_start\"]\n",
        "            targets_end = d[\"targets_end\"]\n",
        "            offsets = d[\"offsets\"].numpy()\n",
        "\n",
        "            ids = ids.to(device, dtype=torch.long)\n",
        "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "            targets_start = targets_start.to(device, dtype=torch.long)\n",
        "            targets_end = targets_end.to(device, dtype=torch.long)\n",
        "\n",
        "            outputs_start, outputs_end = model(   \n",
        "                ids=ids,\n",
        "                mask=mask,\n",
        "                token_type_ids=token_type_ids\n",
        "            )\n",
        "            \n",
        "            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
        "            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
        "\n",
        "            jaccard_scores = []\n",
        "            for px, tweet in enumerate(orig_tweet):\n",
        "                selected_tweet = orig_selected[px]\n",
        "                tweet_sentiment = sentiment[px]\n",
        "                idx_start, idx_end = get_best_start_end_idxs(outputs_start[px, :], outputs_end[px, :])\n",
        "                jaccard_score, output_sentence = calculate_jaccard_score(\n",
        "                    original_tweet=tweet,\n",
        "                    target_string=selected_tweet,\n",
        "                    sentiment_val=tweet_sentiment,\n",
        "                    idx_start=idx_start,\n",
        "                    idx_end=idx_end,\n",
        "                    offsets=offsets[px]\n",
        "                )\n",
        "                jaccard_scores.append(jaccard_score)\n",
        "                final_output.append(output_sentence)\n",
        "\n",
        "            jaccards.append(np.mean(jaccard_scores))\n",
        "            losses.append(0.0)\n",
        "    \n",
        "    return np.mean(losses), np.mean(jaccards), final_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDiDyFVF4RgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.cls = classes\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(dim=self.dim)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
        "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
        "\n",
        "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
        "    loss_fct = LabelSmoothingLoss(config.MAX_LEN, 0.2)\n",
        "\n",
        "    start_loss = loss_fct(start_logits, start_positions)\n",
        "    end_loss = loss_fct(end_logits, end_positions)\n",
        "\n",
        "    total_loss = start_loss + end_loss\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "class RoBERTaForTweet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RoBERTaForTweet, self).__init__()\n",
        "\n",
        "        conf = RobertaConfig.from_pretrained(config.MODEL_PATH)\n",
        "        conf.output_hidden_states = True\n",
        "\n",
        "        self.ksizes = [1, 2, 3]\n",
        "\n",
        "        self.roberta = RobertaModel.from_pretrained(config.MODEL_PATH, config=conf)\n",
        "        self.dropouts = nn.ModuleList([nn.Dropout(0.2) for _ in range(5)])\n",
        "        self.qa_outputs = nn.ModuleList([nn.Conv1d(768, 2, k) for k in self.ksizes])\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, _, out = self.roberta(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
        "        \n",
        "        lst = []\n",
        "        for k, qa_outputs in zip(self.ksizes, self.qa_outputs):\n",
        "            sequence_output = out[-1].transpose(1, 2)\n",
        "            if k > 1:\n",
        "                sequence_output = F.pad(sequence_output, (0, k-1))\n",
        "            h = sum([qa_outputs(dropout(sequence_output)).transpose(1, 2) for dropout in self.dropouts])/5\n",
        "            lst.append(h)\n",
        "        logits = sum(lst)\n",
        "        \n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        return start_logits, end_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5jZPxi5ypvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for l in syn.lemmas():\n",
        "            w = l.name().lower()\n",
        "            if w == word:\n",
        "                continue\n",
        "            synonyms.add(w)\n",
        "    return list(synonyms)\n",
        "\n",
        "def get_augument_df(df, k, n):\n",
        "    lst = []\n",
        "    sample_idx = random.choices(list(range(len(df))), k=k)\n",
        "    for idx in sample_idx:\n",
        "        text = df.loc[idx, \"text\"]\n",
        "        selected_text = df.loc[idx, \"selected_text\"]\n",
        "        sentiment = df.loc[idx, \"sentiment\"]\n",
        "        org_sentiment = df.loc[idx, \"org_sentiment\"]\n",
        "\n",
        "        try:\n",
        "            words = random.sample(text.split(), n)\n",
        "        except ValueError:\n",
        "            continue\n",
        "        if len(words) < n:\n",
        "            continue\n",
        "        replace_dict = {}\n",
        "        for word in words:\n",
        "            word = word.lower()\n",
        "            synonyms = get_synonyms(word)\n",
        "            if len(synonyms) == 0:\n",
        "                continue\n",
        "            replace_word = random.sample(synonyms, 1)[0]\n",
        "            replace_dict[word] = replace_word\n",
        "        if len(replace_dict) < n:\n",
        "            continue\n",
        "\n",
        "        _lst = []\n",
        "        for w in text.split():\n",
        "            try:\n",
        "                w = replace_dict[w.lower()]\n",
        "            except KeyError:\n",
        "                pass\n",
        "            _lst.append(w)\n",
        "        aug_text = \" \".join(_lst)\n",
        "\n",
        "        _lst = []\n",
        "        for w in selected_text.split():\n",
        "            try:\n",
        "                w = replace_dict[w.lower()]\n",
        "            except KeyError:\n",
        "                pass\n",
        "            _lst.append(w)\n",
        "        aug_selected_text = \" \".join(_lst)\n",
        "\n",
        "        d = (aug_text, aug_selected_text, sentiment, org_sentiment)\n",
        "        lst.append(d)\n",
        "\n",
        "    df_aug = pd.DataFrame(lst, columns=[\"text\", \"selected_text\", \"sentiment\", \"org_sentiment\"])\n",
        "    return df_aug"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHGhlrgRcdYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accept_one_words_h = [\"i\", \"a\", \"u\"]\n",
        "def fix_selected_text_header(selected_text):\n",
        "    words = selected_text.lower().split()\n",
        "    head_i = 1 if len(words[0]) == 1 and words[0] not in accept_one_words_h and not words[0].isdigit() else 0\n",
        "    selected_text = \" \".join(selected_text.split()[head_i:])\n",
        "    return selected_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qyh4_wrFTx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fix_selected_text_tail(text, selected_text):\n",
        "    if text.split() == selected_text.split():\n",
        "        return selected_text\n",
        "\n",
        "    last_token = selected_text.split()[-1]\n",
        "    if last_token in text.split():\n",
        "        return selected_text\n",
        "\n",
        "    sp_text = text.split(selected_text)\n",
        "    back_text = sp_text[-1]\n",
        "    if back_text == \"\" or back_text[0] == \" \":\n",
        "        return selected_text\n",
        "\n",
        "    tail_token = back_text.split()[0]\n",
        "    if not tail_token.isalpha():\n",
        "        return selected_text\n",
        "        \n",
        "    _selected_text = selected_text + tail_token\n",
        "\n",
        "\n",
        "    last_token = _selected_text.split()[-1]\n",
        "    if last_token.isalpha():\n",
        "        return _selected_text\n",
        "    if last_token[:4] == \"http\" or \"www.\" in last_token:\n",
        "        return _selected_text\n",
        "    \n",
        "    sp_last_token =  [w for w in last_token.split(\".\") if w != \"\"]\n",
        "    if len(sp_last_token) == 1:\n",
        "        return _selected_text\n",
        "\n",
        "    for s in sp_last_token[1:]:\n",
        "        _selected_text = _selected_text.replace(s, \"\")\n",
        "\n",
        "    return _selected_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QPe-WzW1JoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fix_header_token_2(text, selected_text):\n",
        "    header_token = selected_text.split()[0]\n",
        "    split_header = (\" \"+text).split(header_token)[0]\n",
        "    if split_header != \" \" and header_token not in text.split() and split_header[-1] != \" \" and len(selected_text_fixed.split()) != 1:\n",
        "        return \" \".join(selected_text_fixed.split()[1:])\n",
        "    else:\n",
        "        return selected_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moUYtjxj4Tbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "skf = StratifiedKFold(n_splits=config.N_FOLDS, shuffle=True, random_state=config.SEED).split(train[\"sentiment\"].values, train[\"sentiment\"].values)\n",
        "\n",
        "train_log_dfs, valid_log_dfs = [], []\n",
        "final_outputs = []\n",
        "for fold, (train_idx, valid_idx) in enumerate(skf):\n",
        "    print(f\"### Fold-{fold} ###\")\n",
        "    #if fold in [0, 1, 2, 3]:\n",
        "    #    continue\n",
        "    seed_everything(config.SEED + fold)\n",
        "\n",
        "    df_train = train.iloc[train_idx].reset_index(drop=True)\n",
        "    df_valid = train.iloc[valid_idx].reset_index(drop=True)\n",
        "\n",
        "    df_train[\"selected_text\"] = df_train.apply(lambda x: fix_selected_text_tail(x[\"text\"], x[\"selected_text\"]), axis=1)\n",
        "    df_train[\"selected_text\"] = df_train[\"selected_text\"].map(fix_selected_text_header)\n",
        "    df_train = df_train[df_train[\"selected_text\"] != \"\"].reset_index(drop=True)\n",
        "    df_train[\"selected_text\"] = df_train[\"selected_text\"].map(lambda x: x[:-1] if x[-1] == \",\" else x)\n",
        "    df_train[\"selected_text\"] = df_train.apply(lambda x: fix_header_token_2(x[\"text\"], x[\"selected_text\"]), axis=1)\n",
        "\n",
        "\n",
        "    #df_aug = get_augument_df(df_train, 20000, 2)\n",
        "    #df_train = pd.concat([df_train, df_aug]).reset_index(drop=True)\n",
        "    df_train = pd.concat([df_train, pseudo[[\"text\", \"selected_text\", \"sentiment\", \"org_sentiment\"]]], axis=0).reset_index(drop=True)\n",
        "    #df_train = pd.concat([df_train, big_data_df[[\"text\", \"selected_text\", \"sentiment\", \"org_sentiment\"]]], axis=0).reset_index(drop=True)\n",
        "\n",
        "    train_dataset = TweetDataset(\n",
        "        tweet=df_train.text.values,\n",
        "        sentiment=df_train.sentiment.values,\n",
        "        selected_text=df_train.selected_text.values,\n",
        "        org_sentiment=df_train.org_sentiment.values\n",
        "    )\n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.TRAIN_BATCH_SIZE,\n",
        "        num_workers=0,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    valid_dataset = TweetDataset(\n",
        "        tweet=df_valid.text.values,\n",
        "        sentiment=df_valid.sentiment.values,\n",
        "        selected_text=df_valid.selected_text.values,\n",
        "        org_sentiment=df_valid.org_sentiment.values\n",
        "    )\n",
        "\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=config.VALID_BATCH_SIZE,\n",
        "        num_workers=0,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    model = RoBERTaForTweet()\n",
        "    model.to(device)\n",
        "\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_parameters, lr=config.LR)\n",
        "    \n",
        "    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n",
        "    \n",
        "\n",
        "    t_res, v_res = [], []\n",
        "    best_score = 0\n",
        "    best_final_output = []\n",
        "    for epoch in range(config.EPOCHS):\n",
        "        train_loss, lrs = train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n",
        "        valid_loss, valid_jaccard, final_output = eval_fn(valid_data_loader, model, device)\n",
        "        print(f\"{epoch} epoch, jaccard={valid_jaccard}\")\n",
        "        if best_score < valid_jaccard:\n",
        "            print(f\"  --> Best Model Update!!\")\n",
        "            best_score = valid_jaccard\n",
        "            best_final_output = final_output\n",
        "            torch.save(model.state_dict(), f\"{config.OUT_DIR}/roberta_f{fold}_best.bin\")\n",
        "        t_res.append(pd.DataFrame(zip(train_loss, lrs), columns=[\"train_loss\", \"lrs\"]))\n",
        "        v_res.append((valid_loss, valid_jaccard))\n",
        "\n",
        "    final_outputs.append(best_final_output)\n",
        "    train_log_dfs.append(pd.concat(t_res, axis=0).reset_index(drop=True))\n",
        "    valid_log_dfs.append(pd.DataFrame(v_res, columns=[\"valid_loss\", \"jaccard_score\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDZD2jMw5LLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"skf = StratifiedKFold(n_splits=config.N_FOLDS, shuffle=True, random_state=config.SEED).split(train[\"sentiment\"].values, train[\"sentiment\"].values)\n",
        "\n",
        "final_outputs = []\n",
        "for fold, (train_idx, valid_idx) in enumerate(skf):\n",
        "    print(f\"### Fold-{fold} ###\")\n",
        "    seed_everything(config.SEED + fold)\n",
        "\n",
        "    df_valid = train.iloc[valid_idx].reset_index(drop=True)\n",
        "\n",
        "    valid_dataset = TweetDataset(\n",
        "        tweet=df_valid.text.values,\n",
        "        sentiment=df_valid.sentiment.values,\n",
        "        selected_text=df_valid.selected_text.values,\n",
        "        org_sentiment=df_valid.org_sentiment.values\n",
        "    )\n",
        "\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=config.VALID_BATCH_SIZE,\n",
        "        num_workers=0,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    model = RoBERTaForTweet()\n",
        "    model.to(device)\n",
        "\n",
        "    model.load_state_dict(torch.load(f\"{config.OUT_DIR}/roberta_f{fold}_best.bin\"))\n",
        "\n",
        "    valid_loss, valid_jaccard, final_output = eval_fn(valid_data_loader, model, device)\n",
        "    final_outputs.append(final_output)\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSoxLuemhbwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "oof = []\n",
        "skf = StratifiedKFold(n_splits=config.N_FOLDS, shuffle=True, random_state=config.SEED).split(train[\"sentiment\"].values, train[\"sentiment\"].values)\n",
        "for fold, (train_idx, valid_idx) in enumerate(skf):\n",
        "    df_valid = train.iloc[valid_idx].reset_index(drop=True)\n",
        "\n",
        "    df_valid[\"predict\"] = final_outputs[fold]\n",
        "    oof.append(df_valid)\n",
        "oof = pd.concat(oof, axis=0).reset_index(drop=True)\n",
        "\n",
        "oof.to_csv(f\"{config.OUT_DIR}/oof.csv\", index=None)\n",
        "oof.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-dc_nsVhgDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls \"{config.OUT_DIR}\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMubV3KYO5Z-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}